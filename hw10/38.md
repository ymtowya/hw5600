## Chapter 38

## Q1

*Q: Use the simulator to perform some basic RAID mapping tests. Run
with different levels (0, 1, 4, 5) and see if you can figure out the
mappings of a set of requests. For RAID-5, see if you can figure out
the difference between left-symmetric and left-asymmetric layouts.
Use some different random seeds to generate different problems
than above.*

A: 

For RAID - 0,

disk = (address) % #(disks) = addr % 4

offset = (address) / 4

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 0, offset 4]

8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 0, offset 2]

10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 2, offset 2]

15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 3, offset 3]

9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 1, offset 2]
```

For RAID -1, the disk to store is

disk = (2 * addr) % #(disk) or this value + 1 (the mirror)

offset = (2 * addr) / #(disk)

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 1 -n 5 -R 20 -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 0, offset 8]

8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 0, offset 4]

10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 1, offset 5]

15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 3, offset 7]

9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 2, offset 4]
```

For RAID - 4,

disk = (addr) % (#(disk) - 1)

offset = (addr) / (#(disk) - 1)

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 4 -n 5 -R 20 -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 1, offset 5]
8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 2, offset 2]
10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 1, offset 3]
15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 0, offset 5]
9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 0, offset 3]
```

For RAID -5
The difference between LS and LA is
[The pic](https://img-my.csdn.net/uploads/201004/30/0_1272653659Dx6M.gif)

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 5 -5 LS -n 5 -R 20 -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 0, offset 5]
8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 0, offset 2]
10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 2, offset 3]
15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 3, offset 5]
9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 1, offset 3]

PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 5 -5 LA -n 5 -R 20 -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 1, offset 5]
8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 3, offset 2]
10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 2, offset 3]
15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 0, offset 5]
9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 1, offset 3]
```

## Q2

*Q:Do the same as the first problem, but this time vary the chunk size
with -C. How does chunk size change the mappings?*

A: They will put the blocks to fill each chunk, so block 0 and 1 are both on disk 0's first chunk.

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -C 8k -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 0, offset 4]

8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 0, offset 2]

10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 1, offset 2]

15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 3, offset 3]

9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 0, offset 3]
```

## Q3

*Q:Do the same as above, but use the -r flag to reverse the nature of
each problem.*

A: 
addr = No.Disk + offset * #(disks)

```
16 1
LOGICAL OPERATION is ?
  read  [disk 0, offset 4]

8 1
LOGICAL OPERATION is ?
  read  [disk 0, offset 2]

10 1
LOGICAL OPERATION is ?
  read  [disk 2, offset 2]

15 1
LOGICAL OPERATION is ?
  read  [disk 3, offset 3]

9 1
LOGICAL OPERATION is ?
  read  [disk 1, offset 2]

PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -r -c
16 1
LOGICAL READ from addr:16 size:4096
  read  [disk 0, offset 4]

8 1
LOGICAL READ from addr:8 size:4096
  read  [disk 0, offset 2]

10 1
LOGICAL READ from addr:10 size:4096
  read  [disk 2, offset 2]

15 1
LOGICAL READ from addr:15 size:4096
  read  [disk 3, offset 3]

9 1
LOGICAL READ from addr:9 size:4096
  read  [disk 1, offset 2]
```

## Q4

*Q:Now use the reverse flag but increase the size of each request with
the -S flag. Try specifying sizes of 8k, 12k, and 16k, while varying
the RAID level. What happens to the underlying I/O pattern when
the size of the request increases? Make sure to try this with the
sequential workload too (-W sequential); for what request sizes
are RAID-4 and RAID-5 much more I/O efficient?*

A:

Two blocks for each time.

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -r -S 8k -W seq
0 2
LOGICAL OPERATION is ?
  read  [disk 0, offset 0]
  read  [disk 1, offset 0]

2 2
LOGICAL OPERATION is ?
  read  [disk 2, offset 0]
  read  [disk 3, offset 0]

4 2
LOGICAL OPERATION is ?
  read  [disk 0, offset 1]
  read  [disk 1, offset 1]

6 2
LOGICAL OPERATION is ?
  read  [disk 2, offset 1]
  read  [disk 3, offset 1]

8 2
LOGICAL OPERATION is ?
  read  [disk 0, offset 2]
  read  [disk 1, offset 2]
```

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -r -S 8k -W seq -c
0 2
LOGICAL READ from addr:0 size:8192
  read  [disk 0, offset 0]
  read  [disk 1, offset 0]

2 2
LOGICAL READ from addr:2 size:8192
  read  [disk 2, offset 0]
  read  [disk 3, offset 0]

4 2
LOGICAL READ from addr:4 size:8192
  read  [disk 0, offset 1]
  read  [disk 1, offset 1]

6 2
LOGICAL READ from addr:6 size:8192
  read  [disk 2, offset 1]
  read  [disk 3, offset 1]

8 2
LOGICAL READ from addr:8 size:8192
  read  [disk 0, offset 2]
  read  [disk 1, offset 2]
```

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 0 -n 5 -R 20 -r -S 16k -W seq -c

0 4
LOGICAL READ from addr:0 size:16384
  read  [disk 0, offset 0]
  read  [disk 1, offset 0]
  read  [disk 2, offset 0]
  read  [disk 3, offset 0]

4 4
LOGICAL READ from addr:4 size:16384
  read  [disk 0, offset 1]
  read  [disk 1, offset 1]
  read  [disk 2, offset 1]
  read  [disk 3, offset 1]

8 4
LOGICAL READ from addr:8 size:16384
  read  [disk 0, offset 2]
  read  [disk 1, offset 2]
  read  [disk 2, offset 2]
  read  [disk 3, offset 2]

12 4
LOGICAL READ from addr:12 size:16384
  read  [disk 0, offset 3]
  read  [disk 1, offset 3]
  read  [disk 2, offset 3]
  read  [disk 3, offset 3]

16 4
LOGICAL READ from addr:16 size:16384
  read  [disk 0, offset 4]
  read  [disk 1, offset 4]
  read  [disk 2, offset 4]
  read  [disk 3, offset 4]
```

The larger the request is, the RAID-4/5 can better use its consecutive blocks.

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 1 -n 5 -R 20 -r -S 16k -W seq -c

0 4
LOGICAL READ from addr:0 size:16384
  read  [disk 0, offset 0]
  read  [disk 2, offset 0]
  read  [disk 1, offset 1]
  read  [disk 3, offset 1]

4 4
LOGICAL READ from addr:4 size:16384
  read  [disk 0, offset 2]
  read  [disk 2, offset 2]
  read  [disk 1, offset 3]
  read  [disk 3, offset 3]

8 4
LOGICAL READ from addr:8 size:16384
  read  [disk 0, offset 4]
  read  [disk 2, offset 4]
  read  [disk 1, offset 5]
  read  [disk 3, offset 5]

12 4
LOGICAL READ from addr:12 size:16384
  read  [disk 0, offset 6]
  read  [disk 2, offset 6]
  read  [disk 1, offset 7]
  read  [disk 3, offset 7]

16 4
LOGICAL READ from addr:16 size:16384
  read  [disk 0, offset 8]
  read  [disk 2, offset 8]
  read  [disk 1, offset 9]
  read  [disk 3, offset 9]

PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 4 -n 5 -R 20 -r -S 12k -W seq -c

0 3
LOGICAL READ from addr:0 size:12288
  read  [disk 0, offset 0]    read  [disk 1, offset 0]    read  [disk 2, offset 0]
3 3
LOGICAL READ from addr:3 size:12288
  read  [disk 0, offset 1]    read  [disk 1, offset 1]    read  [disk 2, offset 1]
6 3
LOGICAL READ from addr:6 size:12288
  read  [disk 0, offset 2]    read  [disk 1, offset 2]    read  [disk 2, offset 2]
9 3
LOGICAL READ from addr:9 size:12288
  read  [disk 0, offset 3]    read  [disk 1, offset 3]    read  [disk 2, offset 3]
12 3
LOGICAL READ from addr:12 size:12288
  read  [disk 0, offset 4]    read  [disk 1, offset 4]    read  [disk 2, offset 4]

PS E:\proj\cs\hw5600\hw10> python .\raid.py -L 4 -n 5 -R 20 -r -S 16k -W seq -c

0 4
LOGICAL READ from addr:0 size:16384
  read  [disk 0, offset 0]    read  [disk 1, offset 0]    read  [disk 2, offset 0]    read  [disk 0, offset 1]
4 4
LOGICAL READ from addr:4 size:16384
  read  [disk 1, offset 1]    read  [disk 2, offset 1]    read  [disk 0, offset 2]    read  [disk 1, offset 2]
8 4
LOGICAL READ from addr:8 size:16384
  read  [disk 2, offset 2]    read  [disk 0, offset 3]    read  [disk 1, offset 3]    read  [disk 2, offset 3]
12 4
LOGICAL READ from addr:12 size:16384
  read  [disk 0, offset 4]    read  [disk 1, offset 4]    read  [disk 2, offset 4]    read  [disk 0, offset 5]
16 4
LOGICAL READ from addr:16 size:16384
  read  [disk 1, offset 5]    read  [disk 2, offset 5]    read  [disk 0, offset 6]    read  [disk 1, offset 6]

```

## Q5

*Q:Use the timing mode of the simulator (-t) to estimate the performance of 100 random reads to the RAID, while varying the RAID
levels, using 4 disks.*

A:

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -L 0
disk:0  busy:  58.70  I/Os:    18 (sequential:1 nearly:15 random:2)
disk:1  busy:  94.65  I/Os:    26 (sequential:0 nearly:25 random:1)
disk:2  busy: 100.00  I/Os:    31 (sequential:2 nearly:28 random:1)
disk:3  busy:  86.17  I/Os:    25 (sequential:0 nearly:24 random:1)

STAT totalTime 112.1

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -L 1
disk:0  busy:  53.22  I/Os:    18 (sequential:0 nearly:13 random:5)
disk:1  busy: 100.00  I/Os:    31 (sequential:0 nearly:19 random:12)
disk:2  busy:  94.43  I/Os:    26 (sequential:0 nearly:18 random:8)
disk:3  busy:  86.39  I/Os:    25 (sequential:0 nearly:17 random:8)

STAT totalTime 186.59999999999997

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -L 4
disk:0  busy:  85.70  I/Os:    30 (sequential:0 nearly:24 random:6)
disk:1  busy: 100.00  I/Os:    33 (sequential:0 nearly:26 random:7)
disk:2  busy:  86.31  I/Os:    37 (sequential:1 nearly:32 random:4)
disk:3  busy:   0.00  I/Os:     0 (sequential:0 nearly:0 random:0)

STAT totalTime 195.79999999999998

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -L 5
disk:0  busy:  56.58  I/Os:    18 (sequential:1 nearly:13 random:4)
disk:1  busy:  95.78  I/Os:    26 (sequential:0 nearly:22 random:4)
disk:2  busy: 100.00  I/Os:    31 (sequential:2 nearly:26 random:3)
disk:3  busy:  87.05  I/Os:    25 (sequential:0 nearly:23 random:2)

STAT totalTime 142.09999999999997
```

## Q6

*Q:Do the same as above, but increase the number of disks. How does
the performance of each RAID level scale as the number of disks
increases?*

A:

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -D 8 -L 0
disk:0  busy:  73.00  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:1  busy:  62.00  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:2  busy:  75.25  I/Os:    16 (sequential:1 nearly:14 random:1)
disk:3  busy:  78.25  I/Os:    10 (sequential:0 nearly:9 random:1)
disk:4  busy:  38.75  I/Os:     7 (sequential:0 nearly:6 random:1)
disk:5  busy:  84.75  I/Os:    15 (sequential:1 nearly:13 random:1)
disk:6  busy: 100.00  I/Os:    15 (sequential:0 nearly:14 random:1)
disk:7  busy:  72.25  I/Os:    15 (sequential:0 nearly:14 random:1)

STAT totalTime 40.00000000000001

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -D 8 -L 1
disk:0  busy:  67.14  I/Os:    11 (sequential:0 nearly:9 random:2)
disk:1  busy:  29.86  I/Os:     7 (sequential:0 nearly:6 random:1)
disk:2  busy:  56.43  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:3  busy:  82.43  I/Os:    15 (sequential:0 nearly:14 random:1)
disk:4  busy:  71.57  I/Os:    16 (sequential:0 nearly:15 random:1)
disk:5  busy: 100.00  I/Os:    15 (sequential:0 nearly:13 random:2)
disk:6  busy:  75.00  I/Os:    10 (sequential:0 nearly:9 random:1)
disk:7  busy:  68.14  I/Os:    15 (sequential:0 nearly:14 random:1)

STAT totalTime 70.0

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -D 8 -L 4
disk:0  busy:  67.32  I/Os:    19 (sequential:0 nearly:18 random:1)
disk:1  busy: 100.00  I/Os:    24 (sequential:1 nearly:22 random:1)
disk:2  busy:  34.64  I/Os:     9 (sequential:0 nearly:8 random:1)
disk:3  busy:  31.17  I/Os:     7 (sequential:0 nearly:6 random:1)
disk:4  busy:  70.78  I/Os:    16 (sequential:0 nearly:15 random:1)
disk:5  busy:  56.78  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:6  busy:  43.52  I/Os:    14 (sequential:1 nearly:12 random:1)
disk:7  busy:   0.00  I/Os:     0 (sequential:0 nearly:0 random:0)

STAT totalTime 66.39999999999999

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -R 500 -W rand -c -t -D 8 -L 5
disk:0  busy:  72.11  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:1  busy:  60.54  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:2  busy:  75.06  I/Os:    16 (sequential:1 nearly:14 random:1)
disk:3  busy:  77.55  I/Os:    10 (sequential:0 nearly:9 random:1)
disk:4  busy:  36.73  I/Os:     7 (sequential:0 nearly:6 random:1)
disk:5  busy:  84.13  I/Os:    15 (sequential:1 nearly:13 random:1)
disk:6  busy: 100.00  I/Os:    15 (sequential:0 nearly:14 random:1)
disk:7  busy:  71.20  I/Os:    15 (sequential:0 nearly:14 random:1)

STAT totalTime 44.10000000000001
```

## Q7

*Q:Do the same as above, but use all writes (-w 100) instead of reads.
How does the performance of each RAID level scale now? Can you
do a rough estimate of the time it will take to complete the workload
of 100 random writes?*

A:

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -L 0
disk:0  busy:  58.70  I/Os:    18 (sequential:1 nearly:15 random:2)
disk:1  busy:  94.65  I/Os:    26 (sequential:0 nearly:25 random:1)
disk:2  busy: 100.00  I/Os:    31 (sequential:2 nearly:28 random:1)
disk:3  busy:  86.17  I/Os:    25 (sequential:0 nearly:24 random:1)

STAT totalTime 112.1

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -L 1
disk:0  busy: 100.00  I/Os:    49 (sequential:0 nearly:28 random:21)
disk:1  busy: 100.00  I/Os:    49 (sequential:0 nearly:28 random:21)
disk:2  busy:  91.35  I/Os:    51 (sequential:0 nearly:37 random:14)
disk:3  busy:  91.35  I/Os:    51 (sequential:0 nearly:37 random:14)

STAT totalTime 342.09999999999997

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -L 4
disk:0  busy:  33.31  I/Os:    60 (sequential:0 nearly:54 random:6)
disk:1  busy:  38.86  I/Os:    66 (sequential:0 nearly:59 random:7)
disk:2  busy:  33.55  I/Os:    74 (sequential:1 nearly:69 random:4)
disk:3  busy: 100.00  I/Os:   200 (sequential:1 nearly:184 random:15)

STAT totalTime 503.80000000000035

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -L 5
disk:0  busy:  94.47  I/Os:   102 (sequential:1 nearly:90 random:11)
disk:1  busy:  80.63  I/Os:    90 (sequential:0 nearly:87 random:3)
disk:2  busy: 100.00  I/Os:   116 (sequential:0 nearly:108 random:8)
disk:3  busy:  78.31  I/Os:    92 (sequential:0 nearly:84 random:8)

STAT totalTime 289.09999999999997
```

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -D 8 -L 0
disk:0  busy:  73.00  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:1  busy:  62.00  I/Os:    11 (sequential:0 nearly:10 random:1)
disk:2  busy:  75.25  I/Os:    16 (sequential:1 nearly:14 random:1)
disk:3  busy:  78.25  I/Os:    10 (sequential:0 nearly:9 random:1)
disk:4  busy:  38.75  I/Os:     7 (sequential:0 nearly:6 random:1)
disk:5  busy:  84.75  I/Os:    15 (sequential:1 nearly:13 random:1)
disk:6  busy: 100.00  I/Os:    15 (sequential:0 nearly:14 random:1)
disk:7  busy:  72.25  I/Os:    15 (sequential:0 nearly:14 random:1)

STAT totalTime 40.00000000000001

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W rand -c -t -D 8 -L 1
disk:0  busy:  58.70  I/Os:    18 (sequential:1 nearly:15 random:2)
disk:1  busy:  58.70  I/Os:    18 (sequential:1 nearly:15 random:2)
disk:2  busy:  94.65  I/Os:    26 (sequential:0 nearly:25 random:1)
disk:3  busy:  94.65  I/Os:    26 (sequential:0 nearly:25 random:1)
disk:4  busy: 100.00  I/Os:    31 (sequential:2 nearly:28 random:1)
disk:5  busy: 100.00  I/Os:    31 (sequential:2 nearly:28 random:1)
disk:6  busy:  86.17  I/Os:    25 (sequential:0 nearly:24 random:1)
disk:7  busy:  86.17  I/Os:    25 (sequential:0 nearly:24 random:1)

STAT totalTime 112.1
```

## Q8

*Q:Run the timing mode one last time, but this time with a sequential workload (-W sequential). How does the performance vary
with RAID level, and when doing reads versus writes? How about
when varying the size of each request? What size should you write
to a RAID when using RAID-4 or RAID-5?*

A: 12k or 16k, better than RAID-1.

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -L 0
disk:0  busy: 100.00  I/Os:    25 (sequential:24 nearly:0 random:1)
disk:1  busy: 100.00  I/Os:    25 (sequential:24 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:    25 (sequential:24 nearly:0 random:1)
disk:3  busy: 100.00  I/Os:    25 (sequential:24 nearly:0 random:1)

STAT totalTime 12.499999999999991

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -L 1
disk:0  busy: 100.00  I/Os:    50 (sequential:49 nearly:0 random:1)
disk:1  busy: 100.00  I/Os:    50 (sequential:49 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:    50 (sequential:49 nearly:0 random:1)
disk:3  busy: 100.00  I/Os:    50 (sequential:49 nearly:0 random:1)

STAT totalTime 14.999999999999982

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -L 4
disk:0  busy: 100.00  I/Os:    68 (sequential:33 nearly:34 random:1)
disk:1  busy:  99.25  I/Os:    66 (sequential:32 nearly:33 random:1)
disk:2  busy:  99.25  I/Os:    66 (sequential:32 nearly:33 random:1)
disk:3  busy: 100.00  I/Os:   200 (sequential:33 nearly:166 random:1)

STAT totalTime 13.399999999999988

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -L 5
disk:0  busy:  99.25  I/Os:    98 (sequential:32 nearly:65 random:1)
disk:1  busy:  99.25  I/Os:    98 (sequential:32 nearly:65 random:1)
disk:2  busy: 100.00  I/Os:   100 (sequential:33 nearly:66 random:1)
disk:3  busy: 100.00  I/Os:   104 (sequential:33 nearly:70 random:1)

STAT totalTime 13.399999999999988
```

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -S 8k -L 4
disk:0  busy: 100.00  I/Os:   133 (sequential:66 nearly:66 random:1)
disk:1  busy: 100.00  I/Os:    67 (sequential:66 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:   133 (sequential:66 nearly:66 random:1)
disk:3  busy: 100.00  I/Os:   199 (sequential:66 nearly:132 random:1)

STAT totalTime 16.69999999999999

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -S 12k -L 4
disk:0  busy: 100.00  I/Os:   100 (sequential:99 nearly:0 random:1)
disk:1  busy: 100.00  I/Os:   100 (sequential:99 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:   100 (sequential:99 nearly:0 random:1)
disk:3  busy: 100.00  I/Os:   100 (sequential:99 nearly:0 random:1)

STAT totalTime 20.000000000000036

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -S 16k -L 4
disk:0  busy: 100.00  I/Os:   201 (sequential:133 nearly:67 random:1)
disk:1  busy:  99.57  I/Os:   133 (sequential:132 nearly:0 random:1)
disk:2  busy:  99.57  I/Os:   199 (sequential:132 nearly:66 random:1)
disk:3  busy: 100.00  I/Os:   267 (sequential:133 nearly:133 random:1)

STAT totalTime 23.400000000000084
```

```
PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -S 12k -L 1
disk:0  busy: 100.00  I/Os:   150 (sequential:149 nearly:0 random:1)
disk:1  busy: 100.00  I/Os:   150 (sequential:149 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:   150 (sequential:149 nearly:0 random:1)
disk:3  busy: 100.00  I/Os:   150 (sequential:149 nearly:0 random:1)

STAT totalTime 25.000000000000107

PS E:\proj\cs\hw5600\hw10> python .\raid.py -n 100 -w 100 -R 500 -W seq -c -t -S 16k -L 1
disk:0  busy: 100.00  I/Os:   200 (sequential:199 nearly:0 random:1)
disk:1  busy: 100.00  I/Os:   200 (sequential:199 nearly:0 random:1)
disk:2  busy: 100.00  I/Os:   200 (sequential:199 nearly:0 random:1)
disk:3  busy: 100.00  I/Os:   200 (sequential:199 nearly:0 random:1)

STAT totalTime 30.000000000000178
```
