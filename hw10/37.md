## Chapter 37

## Q1

*Q:Compute the seek, rotation, and transfer times for the following sets of requests: -a 0, -a 6, -a 30, -a 7,30,8, and finally -a 10,11,12,13*

A:

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 0 -c

REQUESTS ['0']

Block:   0  Seek:  0  Rotate:165  Transfer: 30  Total: 195

TOTALS      Seek:  0  Rotate:165  Transfer: 30  Total: 195

REQUESTS ['6']

Block:   6  Seek:  0  Rotate:345  Transfer: 30  Total: 375

TOTALS      Seek:  0  Rotate:345  Transfer: 30  Total: 375

REQUESTS ['30']

Block:  30  Seek: 80  Rotate:265  Transfer: 30  Total: 375

TOTALS      Seek: 80  Rotate:265  Transfer: 30  Total: 375

REQUESTS ['7', '30', '8']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:  30  Seek: 80  Rotate:220  Transfer: 30  Total: 330
Block:   8  Seek: 80  Rotate:310  Transfer: 30  Total: 420

TOTALS      Seek:160  Rotate:545  Transfer: 90  Total: 795

REQUESTS ['10', '11', '12', '13']

Block:  10  Seek:  0  Rotate:105  Transfer: 30  Total: 135
Block:  11  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  12  Seek: 40  Rotate:320  Transfer: 30  Total: 390
Block:  13  Seek:  0  Rotate:  0  Transfer: 30  Total:  30

TOTALS      Seek: 40  Rotate:425  Transfer:120  Total: 585
```

## Q2

*Q: Do the same requests above, but change the seek rate to different values: -S
2, -S 4, -S 8, -S 10, -S 40, -S 0.1. How do the times change?*

A: If you need to seek then the time changes.

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,11,12,13 -c -S 2
REQUESTS ['10', '11', '12', '13']

Block:  10  Seek:  0  Rotate:105  Transfer: 30  Total: 135
Block:  11  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  12  Seek: 20  Rotate:340  Transfer: 30  Total: 390
Block:  13  Seek:  0  Rotate:  0  Transfer: 30  Total:  30

TOTALS      Seek: 20  Rotate:445  Transfer:120  Total: 585
```
```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,30,8 -S 10 -c
REQUESTS ['7', '30', '8']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:  30  Seek:  8  Rotate:292  Transfer: 30  Total: 330
Block:   8  Seek:  8  Rotate: 22  Transfer: 30  Total:  60

TOTALS      Seek: 16  Rotate:329  Transfer: 90  Total: 435
```

## Q3

*Q:Do the same requests above, but change the rotation rate: -R 0.1, -R 0.5,
-R 0.01. How do the times change?*

A: Rotate time & transfer time are longer.


```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,11,12,13 -c -S 2 -R 0.01
REQUESTS ['10', '11', '12', '13']

Block:  10  Seek:  0  Rotate:10499  Transfer:3000  Total:13499
Block:  11  Seek:  0  Rotate:  0  Transfer:3001  Total:3001
Block:  12  Seek: 20  Rotate:35981  Transfer:3000  Total:39001
Block:  13  Seek:  0  Rotate:  0  Transfer:3000  Total:3000

TOTALS      Seek: 20  Rotate:46480  Transfer:12001  Total:58501

PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,30,8 -S 10 -R 0.5 -c
REQUESTS ['7', '30', '8']

Block:   7  Seek:  0  Rotate: 30  Transfer: 60  Total:  90
Block:  30  Seek:  8  Rotate:592  Transfer: 60  Total: 660
Block:   8  Seek:  8  Rotate: 52  Transfer: 60  Total: 120

TOTALS      Seek: 16  Rotate:674  Transfer:180  Total: 870
```

## Q4

*Q:FIFO is not always best, e.g., with the request stream -a 7,30,8, what order should the requests be processed in? Run the shortest seek-time first
(SSTF) scheduler (-p SSTF) on this workload; how long should it take
(seek, rotation, transfer) for each request to be served?
*
A: FIFO just processed based on the order they come in.

Shortest seek will re-arrange them for the shorter seek time job.

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,30,8 -c -p SSTF
REQUESTS ['7', '30', '8']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:   8  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  30  Seek: 80  Rotate:190  Transfer: 30  Total: 300

TOTALS      Seek: 80  Rotate:205  Transfer: 90  Total: 375
```

## Q5

*Q:Now use the shortest access-time first (SATF) scheduler (-p SATF). Does it
make any difference for -a 7,30,8 workload? Find a set of requests where
SATF outperforms SSTF; more generally, when is SATF better than SSTF?*

A: When rotate takes too long time (longer than seeks and back) then it would be better.

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,30,8 -c -p SATF
REQUESTS ['7', '30', '8']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:   8  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  30  Seek: 80  Rotate:190  Transfer: 30  Total: 300

TOTALS      Seek: 80  Rotate:205  Transfer: 90  Total: 375
```

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,5,25 -c -p SATF
REQUESTS ['7', '5', '25']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:  25  Seek: 80  Rotate: 70  Transfer: 30  Total: 180
Block:   5  Seek: 80  Rotate: 10  Transfer: 30  Total: 120

TOTALS      Seek:160  Rotate: 95  Transfer: 90  Total: 345
```
```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 7,5,25 -c -p SSTF
REQUESTS ['7', '5', '25']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:   5  Seek:  0  Rotate:270  Transfer: 30  Total: 300
Block:  25  Seek: 80  Rotate:130  Transfer: 30  Total: 240

TOTALS      Seek: 80  Rotate:415  Transfer: 90  Total: 585
```

## Q6

*Q:Here is a request stream to try: -a 10,11,12,13. What goes poorly when
it runs? Try adding track skew to address this problem (-o skew). Given
the default seek rate, what should the skew be to maximize performance?
What about for different seek rates (e.g., -S 2, -S 4)? In general, could
you write a formula to figure out the skew?*

A: 

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,11,12,13 -c
REQUESTS ['10', '11', '12', '13']

Block:  10  Seek:  0  Rotate:105  Transfer: 30  Total: 135
Block:  11  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  12  Seek: 40  Rotate:320  Transfer: 30  Total: 390
Block:  13  Seek:  0  Rotate:  0  Transfer: 30  Total:  30

TOTALS      Seek: 40  Rotate:425  Transfer:120  Total: 585
```

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,11,12,13 -o 2 -c
REQUESTS ['10', '11', '12', '13']

Block:  10  Seek:  0  Rotate:105  Transfer: 30  Total: 135
Block:  11  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  12  Seek: 40  Rotate: 20  Transfer: 30  Total:  90
Block:  13  Seek:  0  Rotate:  0  Transfer: 30  Total:  30

TOTALS      Seek: 40  Rotate:125  Transfer:120  Total: 285
```

```
Finally, in the default disk, there are 12 sectors per track, meaning that each sector takes up 30 degrees of the rotational space. 

The distance between each track is by default 40 distance units, and the default rate of seeking is 1 distance unit per unit time. Thus, a seek from the outer track to the middle track takes 40 time units.
```
```
skew = (rotaion speed) * (track dist) / (seek) / (sector angle)

when S = 2, skew = 1
S = 4, skew = 1
```

## Q7

*Q: Specify a disk with different density per zone, e.g., -z 10,20,30, which
specifies the angular difference between blocks on the outer, middle, and
inner tracks. Run some random requests (e.g., -a -1 -A 5,-1,0, which
specifies that random requests should be used via the -a -1 flag and that
five requests ranging from 0 to the max be generated), and compute the
seek, rotation, and transfer times. Use different random seeds. What is the
bandwidth (in sectors per unit time) on the outer, middle, and inner tracks?*


A: 

```
First, the rotational speed is by default set to 1 degree per time unit.

So the sectors per unit time is #{sectors} / 360 * 1
```

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -z 10,20,30 -a -1 -A 5,-1,0 -G
REQUESTS [45, 40, 22, 13, 27]

Block:  45  Seek: 40  Rotate:310  Transfer: 20  Total: 370
Block:  40  Seek:  0  Rotate:240  Transfer: 20  Total: 260
Block:  22  Seek: 40  Rotate: 85  Transfer: 10  Total: 135
Block:  13  Seek:  0  Rotate:260  Transfer: 10  Total: 270
Block:  27  Seek:  0  Rotate:130  Transfer: 10  Total: 140

TOTALS      Seek: 80  Rotate:1025  Transfer: 70  Total:1175

PS E:\proj\cs\hw5600\hw10> python ./disk.py -z 10,20,30 -a -1 -A 5,-1,0 -s 2 -G
REQUESTS [51, 51, 3, 4, 45]

Block:  51  Seek: 40  Rotate: 70  Transfer: 20  Total: 130
Block:  51  Seek:  0  Rotate:340  Transfer: 20  Total: 360
Block:   3  Seek: 40  Rotate: 35  Transfer: 10  Total:  85
Block:   4  Seek:  0  Rotate:  0  Transfer: 10  Total:  10
Block:  45  Seek: 40  Rotate: 85  Transfer: 20  Total: 145

TOTALS      Seek:120  Rotate:530  Transfer: 80  Total: 730

```

## Q8

*Q: A scheduling window determines how many requests the disk can examine
at once. Generate random workloads (e.g., -A 1000,-1,0, with different
seeds) and see how long the SATF scheduler takes when the scheduling window is changed from 1 up to the number of requests. How big of a window
is needed to maximize performance? Hint: use the -c flag and donâ€™t turn
on graphics (-G) to run these quickly. When the scheduling window is set
to 1, does it matter which policy you are using?
*

A:

1 is equivalent to FIFO. 

Maximize performance needs window to be size of the disk.

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 1 -p FIFO -c
TOTALS      Seek:20960  Rotate:169165  Transfer:30000  Total:220125
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 1 -p SSTF -c
TOTALS      Seek:20960  Rotate:169165  Transfer:30000  Total:220125
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 1 -p SATF -c
TOTALS      Seek:20960  Rotate:169165  Transfer:30000  Total:220125
```

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 10 -p SSTF -c
TOTALS      Seek:2000  Rotate:47485  Transfer:30000  Total:79485
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 100 -p SSTF -c
TOTALS      Seek:240  Rotate:12285  Transfer:30000  Total:42525
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 1000 -p SSTF -c
TOTALS      Seek: 40  Rotate:6995  Transfer:30000  Total:37035
PS E:\proj\cs\hw5600\hw10> python ./disk.py -A 1000,-1,0 -w 2000 -p SSTF -c
TOTALS      Seek: 40  Rotate:6995  Transfer:30000  Total:37035
```

## Q9

*Q: Create a series of requests to starve a particular request, assuming an SATF
policy. Given that sequence, how does it perform if you use a bounded
SATF (BSATF) scheduling approach? In this approach, you specify the
scheduling window (e.g., -w 4); the scheduler only moves onto the next
window of requests when all requests in the current window have been serviced. Does this solve starvation? How does it perform, as compared to
SATF? In general, how should a disk make this trade-off between performance and starvation avoidance?*
 
A: Not starved, but much longer time.
To see if rotation is too long to wait.

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 23,7,8,9,10,0,1,2,3,4 -p SATF -G
REQUESTS ['23', '7', '8', '9', '10', '0', '1', '2', '3', '4']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:   8  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   9  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  10  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   0  Seek:  0  Rotate: 30  Transfer: 30  Total:  60
Block:   1  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   2  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   3  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   4  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  23  Seek: 40  Rotate:140  Transfer: 30  Total: 210

TOTALS      Seek: 40  Rotate:185  Transfer:300  Total: 525

PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 23,7,8,9,10,0,1,2,3,4 -p BSATF -w 4 -c
REQUESTS ['23', '7', '8', '9', '10', '0', '1', '2', '3', '4']

Block:   7  Seek:  0  Rotate: 15  Transfer: 30  Total:  45
Block:   8  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   9  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:  23  Seek: 40  Rotate:350  Transfer: 30  Total: 420
Block:   2  Seek: 40  Rotate: 20  Transfer: 30  Total:  90
Block:  10  Seek:  0  Rotate:210  Transfer: 30  Total: 240
Block:   0  Seek:  0  Rotate: 30  Transfer: 30  Total:  60
Block:   1  Seek:  0  Rotate:  0  Transfer: 30  Total:  30
Block:   3  Seek:  0  Rotate: 30  Transfer: 30  Total:  60
Block:   4  Seek:  0  Rotate:  0  Transfer: 30  Total:  30

TOTALS      Seek: 80  Rotate:655  Transfer:300  Total:1035
```

## 10

*Q:All the scheduling policies we have looked at thus far are greedy; they pick
the next best option instead of looking for an optimal schedule. Can you
find a set of requests in which greedy is not optimal?*

A: 

```
PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,21 -p SATF -G
REQUESTS ['10', '21']

Block:  21  Seek: 40  Rotate: 35  Transfer: 30  Total: 105
Block:  10  Seek: 40  Rotate:320  Transfer: 30  Total: 390

TOTALS      Seek: 80  Rotate:355  Transfer: 60  Total: 495

PS E:\proj\cs\hw5600\hw10> python ./disk.py -a 10,21 -p FIFO -G
REQUESTS ['10', '21']

Block:  10  Seek:  0  Rotate:105  Transfer: 30  Total: 135
Block:  21  Seek: 40  Rotate:260  Transfer: 30  Total: 330

TOTALS      Seek: 40  Rotate:365  Transfer: 60  Total: 465
```



